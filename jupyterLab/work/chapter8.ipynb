{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np          # 数値計算ライブラリ\n",
    "import matplotlib.pyplot as plt    # グラフ描画ライブラリ\n",
    "import torch                 # 機械学習ライブラリPyTorch\n",
    "from torch import nn        # PyTorchのニューラルネットワークモジュール\n",
    "from torchviz import make_dot      # PyTorchのグラフ可視化ツール\n",
    "import japanize_matplotlib  # matplotlibの日本語表示を可能にするライブラリ\n",
    "import torch.optim as optim # PyTorchの最適化アルゴリズムを定義するoptimモジュール\n",
    "import pandas as pd         # データフレーム処理のためのライブラリ\n",
    "import sklearn              # 機械学習ライブラリscikit-learn\n",
    "from torchinfo import summary   # モデルのサマリー情報を表示するためのライブラリ\n",
    "from sklearn.model_selection import train_test_split  # データのトレーニングとテストの分割を行う関数\n",
    "from sklearn.metrics import accuracy_score   # 正解率を計算するための関数\n",
    "\n",
    "\n",
    "# 以下可視性のために定義。\n",
    "import inspect # フレームを取得するためのライブラリ\n",
    "from IPython.display import display # データフレームを表示するためのライブラリ\n",
    "\n",
    "# 引数の変数名を出力する変数。ただし仕様上、関数を呼び出した場所と同スコープの変数なら1。その一個上なら2，さらにひとつ上なら3にしなければならない。\n",
    "def print_var_name(var,n=1):\n",
    "    # 現在のフレームを取得する\n",
    "    current_frame = inspect.currentframe()\n",
    "    # 現在のフレームのn回外側のフレームを取得する\n",
    "    outer_frame = current_frame\n",
    "    for _ in range(n):\n",
    "        outer_frame =outer_frame.f_back\n",
    "    # 外側のフレームのローカル変数を取得する。2次元タプル?がずらっと配列で並んでいる。\n",
    "    locals_dict = outer_frame.f_locals\n",
    "    # 変数名を取得する\n",
    "    var_name = [k for k, v in locals_dict.items() if v is var]\n",
    "    if not var_name:\n",
    "        print(\"変数が見つかりませんでした。\")\n",
    "    else:\n",
    "        # 変数名を出力する\n",
    "        print(\"変数名 : \",var_name[0],\" 変数型は \",type(var))\n",
    "\n",
    "def dataframe_converter(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        for arg in args:\n",
    "            \n",
    "            try:\n",
    "                print(\"形状は\",arg.shape)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                # 引数がNumPy配列の場合はPandasデータフレームに変換する\n",
    "                print_var_name(arg,2)\n",
    "                if isinstance(arg, np.ndarray):\n",
    "                    df = pd.DataFrame(arg)\n",
    "                    # デコレータで修飾された関数にデータフレームを渡す\n",
    "                    func(df)\n",
    "                elif isinstance(arg, torch.Tensor):\n",
    "                    df = pd.DataFrame(arg.detach().numpy())\n",
    "                    # デコレータで修飾された関数にデータフレームを渡す\n",
    "                    func(df)\n",
    "                elif isinstance(arg, sklearn.utils.Bunch):\n",
    "                    df = pd.DataFrame(arg.data, columns=arg.feature_names)\n",
    "                    # デコレータで修飾された関数にデータフレームを渡す\n",
    "                    func(df)\n",
    "                else:\n",
    "                    # デコレータで修飾された関数にそのままのオブジェクトを渡す\n",
    "                    func(arg)\n",
    "            except:\n",
    "                # デコレータで修飾された関数にそのままのオブジェクトを渡す\n",
    "                func(arg)\n",
    "    return wrapper\n",
    "\n",
    "@dataframe_converter\n",
    "def display_custom(obj,head=True): # 引数のオブジェクトを表示する。可視性を高めるためにpdに変更するがおそらく大規模の場合は乱用はパフォーマンスに影響する。\n",
    "    # 引数のオブジェクトを表示する\n",
    "    if head and isinstance(obj, pd.DataFrame):\n",
    "        display(obj.head())\n",
    "        \n",
    "    else:\n",
    "        display(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイスの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用デバイス：\", torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIStデータセットのダウンロード\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# ダウンロード先のディレクトリを指定\n",
    "data_root = \"./mnist_data\"\n",
    "\n",
    "train_set0 = datasets.MNIST(root=data_root, train=True, download=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_set0))\n",
    "print(len(train_set0)) #__len__メソッドの出力\n",
    "image,label = train_set0[0] #__getitem__メソッドの出力を変数に格納\n",
    "print('入力データの型 : ',type(image))\n",
    "print('正解データの型 : ',type(label))\n",
    "# 画像のサイズとモードを出力\n",
    "print(image.size)  # (640, 480)\n",
    "print(image.mode)  # RGB\n",
    "\n",
    "# (x, y) = (10,10)のピクセルの値を出力\n",
    "pixel_value = image.getpixel((10, 10))\n",
    "print(pixel_value)  \n",
    "\n",
    "# すべてのピクセルの明度値を取得\n",
    "pixel_values = list(image.getdata())\n",
    "\n",
    "# 明度値を表示\n",
    "print(pixel_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set0の属性を出力\n",
    "print(train_set0.train_data.shape)   # torch.Size([60000, 28, 28])\n",
    "print(train_set0.train_labels.shape) # torch.Size([60000])\n",
    "print(train_set0.classes)        # ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力データの画像の表示\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(f'{label}')\n",
    "plt.imshow(image, cmap='gray_r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解データ付きて、最初の20個の画像を表示\n",
    "plt.figure(figsize=(10,3))\n",
    "for i in range(20):\n",
    "    ax=plt.subplot(2,10,i+1)\n",
    "\n",
    "    # imageとlabelを取得\n",
    "    image,label = train_set0[i]\n",
    "\n",
    "    # イメージを表示\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# データの前処理を定義\n",
    "transform1 = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set1 = datasets.MNIST(root=data_root, train=True, download=True, transform=transform1)\n",
    "\n",
    "# 変換結果の確認\n",
    "image,label = train_set1[0]\n",
    "print('変数の型 :',type(train_set1))\n",
    "print('入力データの型 : ',type(image))\n",
    "print('入力データのshape : ',image.shape)\n",
    "print('最小値 : ',image.min())\n",
    "print('最大値 : ',image.max())\n",
    "print('データ数 : ',len(train_set1))\n",
    "print(image[:,10:15,10:15])\n",
    "\n",
    "# 変換前と比較\n",
    "image, _ = train_set0[0] \n",
    "print('変換前:',list(image.crop((10,10,15,15)).getdata()))\n",
    "lst = [x / 255 for x in list(image.crop((10,10,15,15)).getdata())]\n",
    "lst_formatted = ['{:.4f}'.format(x) for x in lst]\n",
    "print(\"正規化 :\",lst_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1~1で正規化をする\n",
    "transform2 = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_set2 = datasets.MNIST(root=data_root, train=True, download=True, transform=transform2)\n",
    "\n",
    "# 変換結果の確認\n",
    "image,label = train_set2[0]\n",
    "print('変数の型 :',type(train_set2))\n",
    "print('入力データの型 : ',type(image))\n",
    "print('入力データのshape : ',image.shape)\n",
    "print('最小値 : ',image.min())\n",
    "print('最大値 : ',image.max())\n",
    "print('データ数 : ',len(train_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力できるように1次元に変換\n",
    "transform3 = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.Lambda(lambda x: x.view(-1))])\n",
    "\n",
    "train_set3 = datasets.MNIST(root=data_root, train=True, download=True, transform=transform3)\n",
    "\n",
    "# 変換結果の確認\n",
    "image,label = train_set3[0]\n",
    "print('変数の型 :',type(train_set3))\n",
    "print('入力データの型 : ',type(image))\n",
    "print('入力データのshape : ',image.shape)\n",
    "print('最小値 : ',image.min())\n",
    "print('最大値 : ',image.max())\n",
    "print('データ数 : ',len(train_set3))\n",
    "print(train_set3.train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上の例より改めて最終的な前処理を定義\n",
    "# データ変換用関数Transform\n",
    "# (1) ToTensor() : PIL形式の画像をPyTorchのテンソルに変換\n",
    "# (2) Normalize() : [0,1]の範囲の画像を[-1,1]の範囲に正規化\n",
    "# (3) Lambda() : 1つ1つのデータを[1,28,28]から[784]に変換\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.Lambda(lambda x: x.view(-1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なデータセットを作成\n",
    "\n",
    "# 訓練データセットを定義\n",
    "train_set = datasets.MNIST(root=data_root, train=True, download=True, transform=transform)\n",
    "\n",
    "# 検証データセットを定義\n",
    "test_set = datasets.MNIST(root=data_root, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_set))\n",
    "print(len(test_set)) #__len__メソッドの出力\n",
    "image,label = test_set[0] #__getitem__メソッドの出力を変数に格納\n",
    "print('入力データの型 : ',type(image))\n",
    "print('正解データの型 : ',type(label))\n",
    "print('入力データのshape : ',image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ミニバッチサイズ\n",
    "batch_size = 500\n",
    "\n",
    "# 訓練用データローダーを作成\n",
    "# 訓練用なのでshuffle=True\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 検証用データローダーを作成\n",
    "# 訓練用ではないのでshuffle=False\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(type(train_loader))\n",
    "\n",
    "# 何組のミニバッチがあるか\n",
    "print(len(train_loader))\n",
    "print(len(train_set)/500) #上記と同じ\n",
    "\n",
    "# データローダーから最初のミニバッチを取り出す\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# ミニバッチの中身を確認\n",
    "print(type(images))\n",
    "print(type(labels))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力次元数\n",
    "n_input = image.shape[0]\n",
    "\n",
    "# 出力次元数=分類先のクラス数\n",
    "n_output = len(train_set0.classes)\n",
    "\n",
    "# 中間層の次元数\n",
    "n_hidden = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "# 784 -> 128 -> 10のにニューラルネットワーク\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_input,n_hidden,n_output):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        #隠れ層の定義\n",
    "        self.l1 = nn.Linear(n_input,n_hidden)\n",
    "\n",
    "        #出力層の定義\n",
    "        self.l2 = nn.Linear(n_hidden,n_output)\n",
    "\n",
    "        # ReLU関数の定義\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数の固定\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# モデル変数の生成\n",
    "net = Net(n_input,n_hidden,n_output)\n",
    "\n",
    "# モデルをGPUに転送\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法の定義\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 学習回数\n",
    "n_epoch = 100\n",
    "\n",
    "# 評価結果を格納するリスト(エポック数、訓練データの損失、テストデータの損失、訓練データの正解率、テストデータの正解率)\n",
    "history = np.zeros((0,5))\n",
    "\n",
    "# モデルのパラメータの確認\n",
    "for parameter in net.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 104/120 [00:06<00:01, 15.14it/s]\n",
      "  0%|          | 14/10000 [02:15<26:56:30,  9.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1204/4069650937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 訓練フェーズ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mn_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# GPUに転送\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tqdmを使って進捗状況を表示\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 繰り返し処理\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "\n",
    "    train_acc, train_loss = 0, 0\n",
    "    test_acc, test_loss = 0, 0\n",
    "    n_train, n_test = 0, 0\n",
    "\n",
    "    # 訓練フェーズ\n",
    "    for inputs,labels in tqdm(train_loader):\n",
    "        n_train += len(inputs)\n",
    "        # GPUに転送\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 勾配の初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 順伝播\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # 損失の計算\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 逆伝播\n",
    "        loss.backward()\n",
    "\n",
    "        # パラメータの更新\n",
    "        optimizer.step()\n",
    "        # 正解率の計算\n",
    "        _, predicted_train = torch.max(outputs.data, 1) #出力の確率分布から最大値があるインデックスを取得しpredicted_trainに格納。つまり予測ラベルとなる。\n",
    "        train_acc += (predicted_train == labels).sum().item()\n",
    "\n",
    "    # 予測フェーズ\n",
    "    for inputs_test, labels_test in test_loader:\n",
    "        n_test += len(inputs_test)\n",
    "        # GPUに転送\n",
    "        inputs_test, labels_test = inputs_test.to(device), labels_test.to(device)\n",
    "\n",
    "        # 順伝播\n",
    "        outputs_test = net(inputs_test)\n",
    "\n",
    "        # 損失の計算\n",
    "        loss_test = criterion(outputs_test, labels_test)\n",
    "\n",
    "        # 正解率の計算\n",
    "        _, predicted = torch.max(outputs_test.data, 1)\n",
    "        test_acc += (predicted == labels_test).sum().item()\n",
    "\n",
    "    # 記録用リストに記録\n",
    "    if (epoch)%100 == 0:\n",
    "        history = np.vstack((history, np.array([[epoch, loss.item(), loss_test.item(), train_acc, test_acc]])))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
